{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Collection\n",
    "\n",
    "Electricity usage data downloaded from PG&E public datasets: https://pge-energydatarequest.com/public_datasets/\n",
    "\n",
    "Weather data scraping from Weather Underground: https://www.wunderground.com/history/monthly/us/ca\n",
    "\n",
    "Spend tens of hours on scraping weather data:\n",
    "\n",
    "1. scraping weather station list from Weather Underground Interactive Map: \n",
    "    1. understand logic: data is requested by tile number, which is determined by scaling level\n",
    "    2. search for PG&E service area and set the bounding tiles\n",
    "    3. scrape station ids, 11141 in total.\n",
    "2. scrape weather data by station id. The data is too big so that this plan was abandoned at first: 11141 stations * 5 years * 12 months * 1 second/request 669,000 seconds = 7.7 days\n",
    "3. explore other 2 apis: NOAA and Open Weather Map\n",
    "    1. NOAA has hundreds of datatypes but only 2 to 3 datatypes are available for the zipcodes, no temperature.\n",
    "    2. Open Weather Map is limited.\n",
    "4. back to Weather Underground again, but use multi-threading to speed up the process. With 100 async concurrent requests, it takes about 3 hours to scrape 11141 stations.\n",
    "\n",
    "After getting json files, the next step is to merge into the main file (all codes have been written):\n",
    "1. combine json files (daily observation) into dataframes (monthly average) and save to local. One dataframe file for one station. (Ongoing: 4 more hours needed)\n",
    "2. convert zipcodes from main file into latitude and longitude.\n",
    "3. find the nearest weather station data for each zipcode for each month.\n",
    "4. for missing data, fill wth 1) same month of nearby years 2) second nearest weather station data"
   ],
   "id": "ce1d288771c04883"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next step: modeling\n",
    "\n",
    "This is a time series forecasting problem, with monthly data, external variables and large dataset.\n",
    "\n",
    "1. baseline model: SARIMA\n",
    "   - ARIMA model is classical time series model, but it is not suitable for seasonal changes.\n",
    "   - Among its variations, SARIMA is the most suitable one for 1) obvious seasonal changes 2) external variables.\n",
    "2. feature engineering based on baseline model result\n",
    "3. main model: LightGBM\n",
    "    - LightGBM is fast and suitable for large dataset.\n",
    "4. model evaluation metrics: MAE and RMSE"
   ],
   "id": "1d8455b19e687b5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Weather data collection",
   "id": "fa41a4a7b1218f1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1 Scrape station ids\n",
    "\n",
    "Manually set tile numbers (x_min=316, x_max=352, y_min=764, y_max=815), according to PG&E serving area."
   ],
   "id": "96c327a28228e9b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def generate_tile_list(x_min=316, x_max=352, y_min=764, y_max=815):\n",
    "    tile_list = []\n",
    "    for x in range(x_min, x_max + 1):\n",
    "        for y in range(y_min, y_max + 1):\n",
    "            tile_list.append((x, y))\n",
    "    return tile_list\n",
    "\n",
    "\n",
    "tiles = generate_tile_list()\n",
    "tile_count = len(tiles)\n",
    "print(f\"Tile count: {tile_count}\")\n",
    "\n",
    "def extract_ids(data):\n",
    "    lis = []\n",
    "    for time_period in data.values():\n",
    "        if isinstance(time_period, dict) and 'features' in time_period:\n",
    "            for feature in time_period['features']:\n",
    "                if 'id' in feature:\n",
    "                    lis.append(feature['id'])\n",
    "    return lis\n",
    "\n",
    "for i, tile in enumerate(tiles):\n",
    "    x, y = tile\n",
    "    url = (f\"https://api3.weather.com/v2/vector-api/products/614/features?x={x}&y={y}\"\n",
    "           f\"&lod=12\"\n",
    "           f\"&apiKey=e1f10a1e78da46f5b10a1e78da96f525\"\n",
    "           f\"&tile-size=512\"\n",
    "           f\"&time=1745128800000-1745129700000:26&time=1745127900000-1745128800000:41&time=1745127000000-1745127900000:56&time=1745126100000-1745127000000:71\"\n",
    "           f\"&stepped=true\")\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"index: \", i)\n",
    "        print(\"Error in tile \", tile)\n",
    "        print(f\"Request failed: {e}\")\n",
    "        break\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print(\"index: \", i)\n",
    "        print(\"Error in tile \", tile)\n",
    "        print(f\"Request failed with status code: {r.status_code}\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        text = json.loads(r.text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"index: \", i)\n",
    "        print(\"Error in tile \", tile)\n",
    "        print(f\"JSON decode failed: {e}\")\n",
    "        print(\"Status code: \", r.status_code)\n",
    "        print(r.text)\n",
    "        break\n",
    "    id_list = extract_ids(text)\n",
    "    print(url)\n",
    "    print(id_list)\n",
    "    # write ids to file\n",
    "    with open(\"data/Weather Station IDs.txt\", \"a\") as f:\n",
    "        for s_id in id_list:\n",
    "            f.write(f\"{s_id}\\n\")\n",
    "    print(f\"Tile {tile} done ({i}/{tile_count})\")\n",
    "    sleep(0.5)"
   ],
   "id": "2837c5fc8cc2f986"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scrape station locations (latitude, longitude)",
   "id": "814379b2b1685994"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "from time import sleep\n",
    "import requests\n",
    "\n",
    "def load_station_ids(file_path):\n",
    "    \"\"\"\n",
    "    Load and deduplicate weather station IDs from a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the text file containing station IDs\n",
    "\n",
    "    Returns:\n",
    "    - ids: list of str, unique sorted station IDs\n",
    "\n",
    "    Sample Usage:\n",
    "    ids = load_station_ids(\"Weather Station IDs.txt\")\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        ids = f.readlines()\n",
    "    ids = [s.strip() for s in ids]\n",
    "    ids = sorted(list(set(ids)))\n",
    "    print(f\"Total loaded station IDs: {len(ids)}\")\n",
    "    return ids\n",
    "\n",
    "def load_completed_stations(file_path):\n",
    "    \"\"\"\n",
    "    Load the set of completed station IDs from an existing CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the CSV file with completed stations\n",
    "\n",
    "    Returns:\n",
    "    - completed: set of str, completed station IDs\n",
    "\n",
    "    Sample Usage:\n",
    "    completed = load_completed_stations(\"station_location.csv\")\n",
    "    \"\"\"\n",
    "    completed = set()\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                completed.add(row[\"s_id\"])\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error reading completed file: {e}\")\n",
    "    return completed\n",
    "\n",
    "def fetch_station_data(s_id):\n",
    "    \"\"\"\n",
    "    Fetch station data from the weather API.\n",
    "\n",
    "    Parameters:\n",
    "    - s_id: str, station ID\n",
    "\n",
    "    Returns:\n",
    "    - data: dict or None, JSON response parsed as a dictionary or None if failed\n",
    "\n",
    "    Sample Usage:\n",
    "    data = fetch_station_data(\"STATION123\")\n",
    "    \"\"\"\n",
    "    url = (f\"https://api.weather.com/v2/pws/observations/current?\"\n",
    "           f\"apiKey=e1f10a1e78da46f5b10a1e78da96f525\"\n",
    "           f\"&stationId={s_id}\"\n",
    "           f\"&numericPrecision=decimal&format=json&units=e\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"{s_id} Request failed: {e}\")\n",
    "        print(f\"URL: {url}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"{s_id} JSON decode failed: {e}\")\n",
    "        print(response.text)\n",
    "    return None\n",
    "\n",
    "def save_station_location(s_id, latitude, longitude, icaoCode, postalCode, file_path):\n",
    "    \"\"\"\n",
    "    Save station location data to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - s_id: str, station ID\n",
    "    - latitude: float, latitude value\n",
    "    - longitude: float, longitude value\n",
    "    - icaoCode: str or None, ICAO code\n",
    "    - postalCode: str or None, postal code\n",
    "    - file_path: str, path to the CSV file\n",
    "\n",
    "    Sample Usage:\n",
    "    save_station_location(\"STATION123\", 40.0, -75.0, \"KABC\", \"12345\", \"station_location.csv\")\n",
    "    \"\"\"\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"{s_id},{latitude},{longitude},{icaoCode},{postalCode}\\n\")\n",
    "\n",
    "def scrape_station_data():\n",
    "    \"\"\"\n",
    "    Main function to process weather station IDs and fetch their location data.\n",
    "    \"\"\"\n",
    "    ids = load_station_ids(\"data/Weather Station IDs.txt\")\n",
    "    completed = load_completed_stations(\"data/station_location.csv\")\n",
    "    tasks = [s_id for s_id in ids if s_id not in completed]\n",
    "    total_tasks = len(ids)\n",
    "    if tasks:\n",
    "        print(f\"[INFO] Total tasks: {total_tasks}, completed: {len(completed)}, remaining: {len(tasks)}\")\n",
    "    \n",
    "    for i, s_id in enumerate(ids):\n",
    "        print(f\"Processing station: {s_id} ({i + 1}/{total_tasks})\")\n",
    "        data = fetch_station_data(s_id)\n",
    "        if data:\n",
    "            for observation in data.get('observations', []):\n",
    "                latitude = observation.get('lat')\n",
    "                longitude = observation.get('lon')\n",
    "                icaoCode = observation.get('icaoCode')\n",
    "                postalCode = observation.get('postalCode')\n",
    "                if latitude is not None and longitude is not None:\n",
    "                    save_station_location(s_id, latitude, longitude, icaoCode, postalCode, \"station_location.csv\")\n",
    "        sleep(0.3)\n",
    "\n",
    "scrape_station_data()"
   ],
   "id": "180bafb055af6680"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2 Scrape weather data by station id",
   "id": "b728b46a92898ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "async def fetch_and_save_weather_data_async(session, year, month, api_key, station_id):\n",
    "    \"\"\"\n",
    "    Asynchronously fetch weather data for a given year and month and save it as a local JSON file.\n",
    "    Skips the request if the file already exists.\n",
    "\n",
    "    Parameters:\n",
    "    - session: aiohttp.ClientSession, aiohttp client session\n",
    "    - year: int, year to request data for\n",
    "    - month: int, month to request data for\n",
    "    - api_key: str, your Weather API key\n",
    "    - station_id: str, weather station ID\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Sample Usage:\n",
    "    await fetch_and_save_weather_data_async(session, 2024, 5, \"APIKEY\", \"STATION123\")\n",
    "    \"\"\"\n",
    "    filename = f\"{station_id}_{year}{month:02d}.json\"\n",
    "    filepath = os.path.join(\"data/weather_json\", filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"[{station_id}] {filepath} already exists, skipping request.\")\n",
    "        return\n",
    "\n",
    "    start_date_str = f\"{year}{month:02d}01\"\n",
    "    if month == 12:\n",
    "        end_year = year\n",
    "        end_month = month\n",
    "        end_day = 31\n",
    "    else:\n",
    "        end_year = year\n",
    "        end_month = month + 1\n",
    "        end_day = 1\n",
    "    end_date = datetime(end_year, end_month, end_day) - timedelta(days=1)\n",
    "    end_date_str = end_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "    api_url = f\"https://api.weather.com/v2/pws/history/daily?stationId={station_id}&format=json&units=e&startDate={start_date_str}&endDate={end_date_str}&numericPrecision=decimal&apiKey={api_key}\"\n",
    "\n",
    "    try:\n",
    "        async with session.get(api_url) as response:\n",
    "            response.raise_for_status()  # Raise HTTPError if request fails\n",
    "            data = await response.json()\n",
    "\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "            print(f\"[{station_id}] Successfully saved data for {year}-{month:02d} to {filepath}\")\n",
    "\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"[{station_id}] aiohttp error while requesting data for {year}-{month:02d}: {e}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"[{station_id}] JSON decode error while parsing response for {year}-{month:02d}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[{station_id}] Unknown error while processing data for {year}-{month:02d}: {e}\")\n",
    "\n",
    "async def process_station(session, api_key, station_id, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Request weather data for all months in the given year range for a single weather station.\n",
    "\n",
    "    Parameters:\n",
    "    - session: aiohttp.ClientSession, aiohttp client session\n",
    "    - api_key: str, your Weather API key\n",
    "    - station_id: str, weather station ID\n",
    "    - start_year: int, start year\n",
    "    - end_year: int, end year\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Sample Usage:\n",
    "    await process_station(session, \"APIKEY\", \"STATION123\", 2020, 2024)\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            task = asyncio.create_task(fetch_and_save_weather_data_async(session, year, month, api_key, station_id))\n",
    "            tasks.append(task)\n",
    "    await asyncio.gather(*tasks)\n",
    "    print(f\"[{station_id}] Completed all year requests.\")\n",
    "\n",
    "async def main(api_key, station_ids, start_year, end_year, concurrency_limit=10):\n",
    "    \"\"\"\n",
    "    Concurrently request weather data for all provided weather stations.\n",
    "\n",
    "    Parameters:\n",
    "    - api_key: str, your Weather API key\n",
    "    - station_ids: list of str, list of weather station IDs\n",
    "    - start_year: int, start year\n",
    "    - end_year: int, end year\n",
    "    - concurrency_limit: int, maximum number of concurrent requests\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Sample Usage:\n",
    "    await main(\"APIKEY\", [\"STATION1\", \"STATION2\"], 2020, 2024, concurrency_limit=10)\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        semaphore = asyncio.Semaphore(concurrency_limit)  # Limit concurrency\n",
    "\n",
    "        async def limited_process_station(station_id):\n",
    "            async with semaphore:\n",
    "                await process_station(session, api_key, station_id, start_year, end_year)\n",
    "\n",
    "        station_tasks = [limited_process_station(station_id) for station_id in station_ids]\n",
    "        await asyncio.gather(*station_tasks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = \"YOUR API HERE\"  # Replace with your actual API key\n",
    "    start_year = 2020\n",
    "    end_year = 2024\n",
    "    concurrency_limit = 100  # Adjust concurrency based on network and API limits\n",
    "\n",
    "    # Read station IDs from CSV file\n",
    "    ids = pd.read_csv(\"data/station_ids.csv\")\n",
    "    station_ids = ids[\"station_id\"].tolist()\n",
    "    # station_ids = [\"KCAANGEL3\"]  # Replace with your list of 11,141 station IDs\n",
    "\n",
    "    # Measure execution time\n",
    "    start_time = time.time()\n",
    "\n",
    "    asyncio.run(main(api_key, station_ids, start_year, end_year, concurrency_limit))\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"All weather station data requests and saves completed, total elapsed time: {elapsed_time:.2f} seconds\")"
   ],
   "id": "6b88b980b54a8485"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 Format Weather data: Daily to Month",
   "id": "dfe719d4db41ed25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import threading\n",
    "import glob\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "def optimized_json_formatter(data: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimized JSON formatter function to convert API response into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: dict, JSON response from the API\n",
    "\n",
    "    Returns:\n",
    "    - df: pd.DataFrame, formatted DataFrame containing observations\n",
    "\n",
    "    Sample Usage:\n",
    "    df = optimized_json_formatter(json_data)\n",
    "    \"\"\"\n",
    "    observations = data.get(\"observations\", [])\n",
    "    if not observations:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    records = []\n",
    "    for obs in observations:\n",
    "        record = {\n",
    "            \"stationID\": obs.get(\"stationID\"),\n",
    "            \"obsTimeLocal\": pd.to_datetime(obs.get(\"obsTimeLocal\")),\n",
    "            \"lat\": obs.get(\"lat\"),\n",
    "            \"lon\": obs.get(\"lon\"),\n",
    "            \"precipRate\": obs.get(\"imperial\", {}).get(\"precipRate\"),\n",
    "        }\n",
    "\n",
    "        # Include fields containing \"avg\" in their names\n",
    "        for key, value in obs.items():\n",
    "            if \"avg\" in key.lower():\n",
    "                record[key] = value\n",
    "        for key, value in obs.get(\"imperial\", {}).items():\n",
    "            if \"avg\" in key.lower():\n",
    "                record[key] = value\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def process_station_data(json_file_pattern, output_dir):\n",
    "    \"\"\"\n",
    "    Process all JSON files for a single weather station, merge them into a DataFrame,\n",
    "    and save as a Feather file.\n",
    "\n",
    "    Parameters:\n",
    "    - json_file_pattern: str, glob pattern matching all JSON files of the station\n",
    "    - output_dir: str, directory to save the Feather file\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Sample Usage:\n",
    "    process_station_data(\"data/weather_json/STATION123_*.json\", \"data/weather_month\")\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    json_files = glob.glob(json_file_pattern)\n",
    "    station_id = None\n",
    "\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            df = optimized_json_formatter(data)\n",
    "            if not df.empty:\n",
    "                all_records.append(df)\n",
    "            if station_id is None and not df.empty:\n",
    "                station_id = df['stationID'].iloc[0]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found {json_file}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: Failed to decode JSON file {json_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing file {json_file}: {e}\")\n",
    "\n",
    "    if all_records and station_id:\n",
    "        combined_df = pd.concat(all_records, ignore_index=True)\n",
    "        output_feather_path = os.path.join(output_dir, f\"{station_id}.feather\")\n",
    "        try:\n",
    "            feather.write_feather(combined_df, output_feather_path)\n",
    "            print(f\"Successfully saved {station_id} data to {output_feather_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while saving Feather file for {station_id}: {e}\")\n",
    "\n",
    "def daily_to_month_main(json_dir, output_feather_dir, num_threads=5):\n",
    "    \"\"\"\n",
    "    Main function to traverse the JSON file directory and create threads for each weather station.\n",
    "\n",
    "    Parameters:\n",
    "    - json_dir: str, directory containing all JSON files\n",
    "    - output_feather_dir: str, directory to save Feather files\n",
    "    - num_threads: int, number of concurrent threads to process data\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Sample Usage:\n",
    "    daily_to_month_main(\"data/weather_json\", \"data/weather_month\", 10)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_feather_dir, exist_ok=True)\n",
    "    station_ids = set()\n",
    "    json_files = glob.glob(os.path.join(json_dir, \"*.json\"))\n",
    "    for file in json_files:\n",
    "        try:\n",
    "            station_id = file.split(os.sep)[-1].split('_')[0]\n",
    "            station_ids.add(station_id)\n",
    "        except Exception:\n",
    "            print(f\"Failed to parse filename: {file}\")\n",
    "\n",
    "    threads = []\n",
    "    for station_id in sorted(list(station_ids)):\n",
    "        output_feather_path = os.path.join(output_feather_dir, f\"{station_id}.feather\")\n",
    "        if os.path.exists(output_feather_path):\n",
    "            print(f\"{output_feather_path} already exists.\")\n",
    "            continue\n",
    "\n",
    "        json_file_pattern = os.path.join(json_dir, f\"{station_id}_*.json\")\n",
    "        thread = threading.Thread(target=process_station_data, args=(json_file_pattern, output_feather_dir))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        if len(threads) >= num_threads:\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "            threads = []\n",
    "\n",
    "    # Wait for any remaining threads to finish\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    print(\"All weather station JSON files have been converted to Feather files.\")\n",
    "\n",
    "json_data_dir = \"data/weather_json\"  # Replace with your JSON file directory\n",
    "output_feather_dir = \"data/weather_month\"  # Replace with your desired Feather file directory\n",
    "num_processing_threads = 50  # Adjust the number of threads based on your CPU cores\n",
    "\n",
    "daily_to_month_main(json_data_dir, output_feather_dir, num_processing_threads)"
   ],
   "id": "3d91422186d8396f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4 Abstract zipcode list",
   "id": "e8e33d87d8be91e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import pgeocode\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Load raw data\n",
    "raw1 = pd.read_csv(\"PGE_2020-2024_Data/PGE_Combined_2020_2024.csv\")  # Assume your raw data is in this CSV file\n",
    "raw = raw1.copy()\n",
    "\n",
    "# Get unique list of ZIP codes\n",
    "zipcodes = raw['ZIPCODE'].unique()\n",
    "\n",
    "def zipcode_to_latlon(zipcode, country='US'):\n",
    "    \"\"\"\n",
    "    Convert a ZIP code to its latitude and longitude using pgeocode.\n",
    "\n",
    "    Parameters:\n",
    "    - zipcode: str or int, the ZIP code to convert\n",
    "    - country: str, country code (default: 'US')\n",
    "\n",
    "    Returns:\n",
    "    - latitude: float or None, the latitude of the ZIP code\n",
    "    - longitude: float or None, the longitude of the ZIP code\n",
    "\n",
    "    Sample Usage:\n",
    "    lat, lon = zipcode_to_latlon('94103')\n",
    "    \"\"\"\n",
    "    nomi = pgeocode.Nominatim(country)\n",
    "    location = nomi.query_postal_code(zipcode)\n",
    "    if location.empty or pd.isnull(location.latitude):\n",
    "        return None, None\n",
    "    return location.latitude, location.longitude\n",
    "\n",
    "def convert_zipcode_to_latlon(zipcodes):\n",
    "    \"\"\"\n",
    "    Convert a list of ZIP codes to a list of (latitude, longitude) tuples.\n",
    "\n",
    "    Parameters:\n",
    "    - zipcodes: list of str or int, ZIP codes to convert\n",
    "\n",
    "    Returns:\n",
    "    - results: list of tuples, each tuple contains (latitude, longitude)\n",
    "\n",
    "    Sample Usage:\n",
    "    results = convert_zipcode_to_latlon(['94103', '10001'])\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for zipcode in tqdm(zipcodes, desc=\"Processing ZIPCODE\"):\n",
    "        lat, lon = zipcode_to_latlon(zipcode)\n",
    "        results.append((lat, lon))\n",
    "    return results\n",
    "\n",
    "# Create a dictionary mapping ZIP codes to (latitude, longitude)\n",
    "zipcode_lat_lon_dict = {}\n",
    "for zipcode in tqdm(zipcodes, desc=\"Processing ZIPCODE\"):\n",
    "    lat, lon = zipcode_to_latlon(zipcode)\n",
    "    if lat is not None and lon is not None:\n",
    "        zipcode_lat_lon_dict[zipcode] = (lat, lon)\n",
    "    else:\n",
    "        print(f\"ZIPCODE {zipcode} could not be converted to latitude and longitude.\")\n",
    "\n",
    "# Save the dictionary to a pickle file\n",
    "with open(\"data/zipcode_lat_lon_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(zipcode_lat_lon_dict, f)\n",
    "\n",
    "print(\"ZIP code to latitude/longitude conversion completed and saved to data/zipcode_lat_lon_dict.pkl.\")"
   ],
   "id": "abb2e54997d0983e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5 Merge monthly weather data and pivot",
   "id": "541f000c6367d6d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "def load_and_merge_feather(input_dir=\"data/weather_month\"):\n",
    "    \"\"\"\n",
    "    Load all Feather files from the specified directory and merge them into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dir: str, path to the directory containing Feather files\n",
    "\n",
    "    Returns:\n",
    "    - merged_df: pd.DataFrame, combined DataFrame of all Feather files; empty DataFrame if none found\n",
    "\n",
    "    Sample Usage:\n",
    "    weather_raw = load_and_merge_feather(\"data/weather_month\")\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    feather_files = glob.glob(os.path.join(input_dir, \"*.feather\"))\n",
    "    print(f\"Found {len(feather_files)} Feather files.\")\n",
    "    for file in feather_files:\n",
    "        try:\n",
    "            df = feather.read_feather(file)\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    if all_dfs:\n",
    "        merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        return merged_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load and merge weather data\n",
    "weather_raw = load_and_merge_feather()\n",
    "\n",
    "if weather_raw.empty:\n",
    "    print(\"No weather data found.\")\n",
    "else:\n",
    "    print(\"Successfully loaded and merged weather data.\")\n",
    "    print(f\"Merged weather data shape: {weather_raw.shape}\")\n",
    "\n",
    "weather = weather_raw.copy()\n",
    "\n",
    "# Prepare monthly weather summary\n",
    "weather_monthly = weather.copy()\n",
    "weather_monthly['year_month'] = weather_monthly['obsTimeLocal'].dt.strftime('%Y-%m')\n",
    "weather_monthly = weather_monthly.drop(columns=['obsTimeLocal'])\n",
    "print(weather_monthly['year_month'].head())\n",
    "\n",
    "# Group by station and month, calculate monthly averages\n",
    "weather_monthly = weather_monthly.groupby(['stationID', 'year_month']).mean(numeric_only=True).reset_index()\n",
    "weather_monthly = weather_monthly[['stationID', 'year_month', 'lat', 'lon'] + [col for col in weather_monthly.columns if col not in ['stationID', 'year_month', 'lat', 'lon']]]\n",
    "print(f\"Weather data after pivoting shape: {weather_monthly.shape}\")\n",
    "print(weather_monthly.head())\n",
    "\n",
    "# Split year_month into separate YEAR and MONTH columns\n",
    "weather_monthly[['YEAR', 'MONTH']] = weather_monthly['year_month'].str.split('-', expand=True)\n",
    "weather_monthly['MONTH'] = weather_monthly['MONTH'].astype(int)\n",
    "weather_monthly = weather_monthly.drop(columns=['year_month'])\n",
    "print(\"Created YEAR and MONTH columns and removed year_month column.\")\n",
    "print(f\"Final weather data shape: {weather_monthly.shape}\")\n",
    "print(weather_monthly.head())\n",
    "\n",
    "# Save the processed monthly weather data to Feather format\n",
    "weather_monthly.to_feather(\"data/weather.feather\")\n",
    "print(\"Saved final monthly weather data to data/weather.feather.\")"
   ],
   "id": "e2d4ae5c2739eb7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6 map stationID to lat, lon",
   "id": "f5c3f4cac86a4b03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pickle\n",
    "\n",
    "# Get unique list of stationID, lat, lon\n",
    "station_loc = weather_monthly[['stationID', 'lat', 'lon']].drop_duplicates()\n",
    "\n",
    "# Create a mapping from stationID to (lat, lon)\n",
    "station_loc_mapping = {}\n",
    "for _, row in station_loc.iterrows():\n",
    "    station_id = row['stationID']\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    station_loc_mapping[station_id] = (lat, lon)\n",
    "\n",
    "# Save the mapping to a pickle file\n",
    "with open(\"data/station_loc_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(station_loc_mapping, f)\n",
    "\n",
    "print(\"Saved stationID to (lat, lon) mapping to data/station_loc_mapping.pkl.\")"
   ],
   "id": "8854ea61dbbc530a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7 Map zipcode to top 10 nearest stationIDs",
   "id": "a7b35056d31a2961"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pickle\n",
    "from geopy.distance import geodesic\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load ZIP code to (lat, lon) mapping\n",
    "with open(\"data/zipcode_lat_lon_dict.pkl\", \"rb\") as f:\n",
    "    zipcode_lat_lon_dict = pickle.load(f)\n",
    "\n",
    "# Load stationID to (lat, lon) mapping\n",
    "with open(\"data/station_loc_mapping.pkl\", \"rb\") as f:\n",
    "    station_loc_mapping = pickle.load(f)\n",
    "\n",
    "def find_top_nearest(lat_lon_pair, station_loc_mapping, top_n):\n",
    "    \"\"\"\n",
    "    Find the top N nearest weather stations to a given (latitude, longitude) pair.\n",
    "\n",
    "    Parameters:\n",
    "    - lat_lon_pair: tuple, (latitude, longitude)\n",
    "    - station_loc_mapping: dict, mapping from stationID to (latitude, longitude)\n",
    "    - top_n: int, number of nearest stations to return\n",
    "\n",
    "    Returns:\n",
    "    - nearest_stations: tuple, top N nearest station IDs\n",
    "\n",
    "    Sample Usage:\n",
    "    nearest = find_top_nearest((37.7749, -122.4194), station_loc_mapping, 5)\n",
    "    \"\"\"\n",
    "    lat, lon = lat_lon_pair\n",
    "    distances = [\n",
    "        (geodesic((lat, lon), coords).km, station_id)  # (distance, station_id)\n",
    "        for station_id, coords in station_loc_mapping.items()\n",
    "    ]\n",
    "    # Get the top N nearest station IDs\n",
    "    return tuple(station_id for _, station_id in sorted(distances, key=lambda x: x[0])[:top_n])\n",
    "\n",
    "def lat_lon_map_to_weather_multithread(zip_lat_lon_mapping,\n",
    "                                       station_loc_mapping,\n",
    "                                       top_n,\n",
    "                                       max_workers=32):\n",
    "    \"\"\"\n",
    "    Map ZIP codes to their nearest weather stations using multithreading.\n",
    "\n",
    "    Parameters:\n",
    "    - zip_lat_lon_mapping: dict, mapping from ZIP code to (latitude, longitude)\n",
    "    - station_loc_mapping: dict, mapping from stationID to (latitude, longitude)\n",
    "    - top_n: int, number of nearest stations to find\n",
    "    - max_workers: int, maximum number of threads\n",
    "\n",
    "    Returns:\n",
    "    - results: dict, mapping from ZIP code to tuple of nearest station IDs\n",
    "\n",
    "    Sample Usage:\n",
    "    results = lat_lon_map_to_weather_multithread(zipcode_lat_lon_dict, station_loc_mapping, 10, 50)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(find_top_nearest, lat_lon, station_loc_mapping, top_n): zipcode\n",
    "            for zipcode, lat_lon in zip_lat_lon_mapping.items()\n",
    "        }\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures),\n",
    "                           total=len(futures), desc=\"Processing ZIPCODE\"):\n",
    "            zipcode = futures[future]\n",
    "            results[zipcode] = future.result()  # tuple of nearest station IDs\n",
    "    return results\n",
    "\n",
    "# Run the mapping process\n",
    "near_lat_lon = lat_lon_map_to_weather_multithread(\n",
    "    zipcode_lat_lon_dict, station_loc_mapping, top_n=10, max_workers=50)\n",
    "\n",
    "# Save the results to a pickle file\n",
    "with open(\"data/near_lat_lon.pkl\", \"wb\") as f:\n",
    "    pickle.dump(near_lat_lon, f)\n",
    "\n",
    "print(\"Saved ZIP code to nearest station mapping to data/near_lat_lon.pkl.\")\n"
   ],
   "id": "3c12494f72c6c6ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8 Merge weather data with main data",
   "id": "53a2211df9296d6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "with open(\"data/near_lat_lon.pkl\", \"rb\") as f:\n",
    "    near_lat_lon = pickle.load(f)\n",
    "weather = pd.read_feather(\"data/weather.feather\")\n",
    "raw = pd.read_csv(\"PGE_2020-2024_Data/PGE_Combined_2020_2024.csv\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, List\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def fill_weather_to_raw(weather: pd.DataFrame, raw: pd.DataFrame, near_lat_lon: Dict[str, Tuple[str]], save_path: str = \"data/merged.feather\") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Fills weather data into raw DataFrame based on nearest weather stations, matching by year and month.\n",
    "    Supports breakpoint continuation by saving progress to a Feather file.\n",
    "    \n",
    "    Args:\n",
    "        weather: DataFrame with weather data including stationID, YEAR, MONTH, and weather metrics\n",
    "        raw: DataFrame with ZIPCODE, YEAR, MONTH, and other columns\n",
    "        near_lat_lon: Dictionary mapping ZIPCODE to tuple of nearest stationIDs (ordered nearest to farthest)\n",
    "        save_path: Path to save the merged DataFrame for breakpoint continuation (default: 'data/merged.feather')\n",
    "    \n",
    "    Returns:\n",
    "        merged_df: Raw DataFrame with added weather columns\n",
    "        zipcodes_missing_weather: List of ZIPCODEs where no weather data could be matched\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    # Weather columns to merge (excluding stationID, lat, lon, YEAR, MONTH)\n",
    "    weather_cols = weather.columns.difference(['stationID', 'lat', 'lon', 'YEAR', 'MONTH']).tolist()\n",
    "    \n",
    "    # Convert YEAR in raw to string for consistency with weather\n",
    "    raw = raw.copy()\n",
    "    raw['YEAR'] = raw['YEAR'].astype(str)\n",
    "    \n",
    "    # Check if a saved Feather file exists for continuation\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Resuming from saved file: {save_path}\")\n",
    "        merged_df = pd.read_feather(save_path)\n",
    "        # Ensure all weather columns exist in the loaded DataFrame\n",
    "        for col in weather_cols:\n",
    "            if col not in merged_df.columns:\n",
    "                merged_df[col] = np.nan\n",
    "    else:\n",
    "        # Initialize output DataFrame\n",
    "        merged_df = raw.copy()\n",
    "        for col in weather_cols:\n",
    "            merged_df[col] = np.nan\n",
    "    \n",
    "    # Track ZIPCODEs with no weather data\n",
    "    zipcodes_missing_weather = []\n",
    "    \n",
    "    # Group weather data by stationID, YEAR, MONTH for efficient lookup\n",
    "    weather_grouped = weather.groupby(['stationID', 'YEAR', 'MONTH'])[weather_cols].mean().reset_index()\n",
    "    \n",
    "    # Identify rows that still need processing (where all weather columns are NaN)\n",
    "    rows_to_process = merged_df[merged_df[weather_cols].isna().all(axis=1)]\n",
    "    \n",
    "    # For each row that needs processing with progress bar\n",
    "    for idx, row in tqdm(rows_to_process.iterrows(), total=rows_to_process.shape[0], desc=\"Processing rows\"):\n",
    "        zipcode = str(row['ZIPCODE'])\n",
    "        year = row['YEAR']\n",
    "        month = row['MONTH']\n",
    "        \n",
    "        if zipcode not in near_lat_lon:\n",
    "            zipcodes_missing_weather.append(zipcode)\n",
    "            continue\n",
    "        \n",
    "        # Get the list of nearest stations\n",
    "        stations = near_lat_lon[zipcode]\n",
    "        weather_found = False\n",
    "        \n",
    "        # Try strategies in order\n",
    "        for station in stations:\n",
    "            # Strategy 1: Same station, same year, same month\n",
    "            match = weather_grouped[\n",
    "                (weather_grouped['stationID'] == station) &\n",
    "                (weather_grouped['YEAR'] == year) &\n",
    "                (weather_grouped['MONTH'] == month)\n",
    "            ]\n",
    "            if not match.empty:\n",
    "                for col in weather_cols:\n",
    "                    merged_df.at[idx, col] = match.iloc[0][col]\n",
    "                weather_found = True\n",
    "                break\n",
    "        \n",
    "        if weather_found:\n",
    "            continue\n",
    "            \n",
    "        # Strategy 2: Same station, near years, same month\n",
    "        for station in stations:\n",
    "            near_years = [str(int(year) + i) for i in [-1, 1, -2, 2]]  # Try ±1, ±2 years\n",
    "            for near_year in near_years:\n",
    "                match = weather_grouped[\n",
    "                    (weather_grouped['stationID'] == station) &\n",
    "                    (weather_grouped['YEAR'] == near_year) &\n",
    "                    (weather_grouped['MONTH'] == month)\n",
    "                ]\n",
    "                if not match.empty:\n",
    "                    for col in weather_cols:\n",
    "                        merged_df.at[idx, col] = match.iloc[0][col]\n",
    "                    weather_found = True\n",
    "                    break\n",
    "            if weather_found:\n",
    "                break\n",
    "        \n",
    "        if weather_found:\n",
    "            continue\n",
    "            \n",
    "        # Strategy 3: Next nearest station, same year, same month, etc.\n",
    "        for station in stations[1:]:  # Skip the first station\n",
    "            match = weather_grouped[\n",
    "                (weather_grouped['stationID'] == station) &\n",
    "                (weather_grouped['YEAR'] == year) &\n",
    "                (weather_grouped['MONTH'] == month)\n",
    "            ]\n",
    "            if not match.empty:\n",
    "                for col in weather_cols:\n",
    "                    merged_df.at[idx, col] = match.iloc[0][col]\n",
    "                weather_found = True\n",
    "                break\n",
    "            if weather_found:\n",
    "                break\n",
    "        \n",
    "        if not weather_found:\n",
    "            zipcodes_missing_weather.append(zipcode)\n",
    "        \n",
    "        # Save progress to Feather file every 100 rows\n",
    "        if idx % 100 == 0:\n",
    "            merged_df.to_feather(save_path)\n",
    "    \n",
    "    # Save final merged DataFrame\n",
    "    merged_df.to_feather(save_path)\n",
    "    \n",
    "    # Remove duplicates from zipcodes_missing_weather\n",
    "    zipcodes_missing_weather = list(set(zipcodes_missing_weather))\n",
    "    \n",
    "    return merged_df, zipcodes_missing_weather\n",
    "\n",
    "merged_df, zipcodes_missing_weather = fill_weather_to_raw(\n",
    "    weather=weather,\n",
    "    # try 5 rows first\n",
    "    raw=raw,\n",
    "    near_lat_lon=near_lat_lon,\n",
    "    save_path=\"data/merged.feather\"\n",
    ")\n",
    "merged_df.head()"
   ],
   "id": "18d9b0094ea9357f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
